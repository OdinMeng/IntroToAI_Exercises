---
geometry: margin=0.75in

---

# Introduction to Artificial Intelligence
## Homework 3 Resolution by Dino Meng [SM3201466]

---

# Q1. Searching with Heuristics
**A1.** $C = h^\ast(S)$, by direct calculations we can estimate $h'(X)$ as 
$$
h'(X)=\frac{1}{2}h(X)\leq \frac{1}{2} h^\ast(X) \leq h^\ast (X)
$$
Meaning that $h'$ is admissible thus gives an optimal solution

**A2.** $C = h^\ast(S)$, since that we can similarly estimate $h'(X)$ as before:
$$
h'(X)=\frac{h(X)+h^\ast(X)}{2}\leq \frac{2h^\ast(X)}{2}=h^\ast(X)
$$
Proving again that $h'$ is admissible.

**A3.** $C \geq h^\ast(S)$, as $h'(X)$ is not admissible. By estimating $h'$ again, we get the bounds
$$
h'(X)\leq h(X)+h^\ast(X)\leq 2h^\ast (X)
$$
Meaning that in certain cases it could be that $2h'(X) \geq h'(X) > h'(X)$, which does not guarantee admissibility for the heuristic.

**A4.** $C \geq h^\ast(S)$, as $h'$ is admissible. To prove that it is admissible, we first can observe that that $h'(X)-h(X) \leq 0$ for any node; the idea is that $h'$ will be lower than $h$ as it tries to find a node of a lower cost in its "network" of neighbouring nodes. We will prove it by induction on $h'$.
- Base case: We assume that $K(X)$ empty, then $h'(X)\leq h(X)$ which is $h'(X)-h(X) \leq 0$
- Inductive Case: Assume that for each $Y \in K(X) \neq \emptyset$ we have that $h'(Y) - h(Y) \leq 0$. Then calculating $h'(X)-h(X)$ we have that
$$
h'(X)-h(X) \leq \min_{y \in K(X)} h'(Y)-h(Y)\leq 0
$$
Then by making few direct calculations on $h$, we can see that:
$$
h'(X) \leq \begin{cases}\overbrace{\min_{Y \in K(X)} h'(Y)-h(Y)}^{\leq 0}+h(X), K(X)\neq \emptyset \\ h(X), K(X)=\emptyset\end{cases} \leq h(X) \leq h^\ast(X)
$$
proving that $h'$ is admissible, concluding.

**A5.**  $C \geq h^\ast(S)$, as $h'$ is admissible. Notice that $\texttt{cost}(x,y) = h^\ast(x)-h^\ast(y)$. Then the heuristic $h'$ can be estimated as follows:

\begin{align}
h'(x)&=\begin{cases}\min_{Y \in K(X)}(h(Y)+\texttt{cost}(X,Y)), K(X)\neq \emptyset \\ 
h(X), K(X)=\emptyset 
\end{cases}
\\
& \leq \begin{cases} \min_{Y \in K(X)}(h^\ast(Y)+h^\ast(X)-h^\ast(Y)), K(X) \neq \emptyset \\ 
h^\ast(X), K(X)=\emptyset
\end{cases} \\
&= \begin{cases} h^\ast(X), K(X) \neq \emptyset \\ 
h^\ast(X), K(X)=\emptyset
\end{cases} = h^\ast(X)
\end{align}

Proving that $h'$ is admissible, concluding.

**A6.**  $C \geq h^\ast(S)$, as $h'$ is admissible. By the properties of the minimum, 
$$
\min_{Y \in K(X)+\{X\} }h(Y) \leq h(X)
$$
So then
$$
h'(X) \leq h(X) \leq h^\ast(X)
$$
proving $h'$ is admissible.

**ii.** The only heuristic which is surely to dominate $h$ is the second one, as we are taking the average between $h$ and $h^\ast$ so since that $h^\ast \geq h$ surely so will the average dominate $h$. The rest are either not admissible or are proven to be dominated by $h$.

**B1.** $h'$ is consistent, to verify it we can make direct calculations and using the fact that $h$ is consistent:

\begin{align}
h'(X) &=\frac{1}{2}h(X) \\
&\leq \frac{1}{2}(c(X,Y)+h(Y)) \\
&=\frac{1}{2}c(X,Y)+\frac{1}{2}h(Y) \\
&= \frac{1}{2}c(X,Y)+h'(Y) \\
&\leq c(X,Y)+h'(Y)
\end{align}

Which is the definition of consistency on $h$', concluding.

**B2.** Similarly to B1, $h'$ is consistent and we can verify it with direct calculations:

\begin{align} h
'(X)&=\frac{h(X)+h^\ast(X)}{2} \\
&\leq \frac{c(X,Y)+h(Y)+h^\ast(X)}{2} \\
&= \frac{h^\ast(X)-h^\ast(Y)+h(Y)+h^\ast(X)+h^\ast(Y)-h^\ast(Y)}{2}\\ 
&= \frac{2c(X,Y)}{2}+h'(Y) \\
&= c(X,Y)+h'(y)
\end{align}

Concluding.

**B3.** $C \geq h^\ast(S)$: If $h'$ is not admissible, then it cannot be consistent (as consistency implies admissibility).

**B4.** $C = h^\ast(S)$: we can see that $h'$ is consistent. To prove it, we can see that by the properties of the minimum that
$$
\forall Y \in K(X), h(X) \leq h'(Y)-h(Y)+h(X)
$$
which implies
$$
h'(Y)\leq h(X)-h(Y) \leq c(X,Y)
$$
So
$$
h'(Y) \leq c(X,Y) + h(X)
$$
Which implies $h'$ is consistent, concluding.

**B5.** $C = h^\ast(S)$: we can see that $h'$ is consistent. Before proving its consistency, let us observe that if $h$ is consistent then $h' \geq h$: in fact we have that

$$
h(X)-h(Y)\leq c(X,Y) \iff h(X) \leq c(X,Y)+h(Y)
$$

Then applying it for $Y  = \min\{Y \in K(X): h(Y)+c(X,Y)\}$, we have that

$$
h(X) \leq h'(X)
$$

Now let us prove the consistency. By definition, if we have that $K(X)\neq 0$ then 

$$
h'(X) \leq h(Y) +c(X,Y), \forall Y \in K(X)
$$

For dominance we have that

$$
h'(X) \leq h'(Y)+c(X,Y)
$$

concluding the proof.

**B6.** We cannot be sure that it is consistent, in fact we have a counterexample for the graph $G = \{S, A, B, G\}$ with weights $SA=4, AB=2, SB=8, BG=42$.

**ii.** $h'$ is consistent and dominant over $h$ for **B2** and **B5**.

**C1.** Bobâ€™s conclusion is wrong, because $h''$ can be inadmissible for $X_0=G$ as it becomes negative for $h''(X_0)$.

---

# Q2. Iterative Deepening Search

**a)**

* i) $+\infty$
* ii) False
* iii) new_limit
* iv) cutoff
* v) min(new_limit, f[child_node])
* vi) True
* vii) new_limit

**b)** The number of times that iterative deepening A\* expands a node is greater than or equal the number of times A\* will expand a node, because the variant runs the original algorithms more times for each different limit value, so it tends to re-explore certain nodes